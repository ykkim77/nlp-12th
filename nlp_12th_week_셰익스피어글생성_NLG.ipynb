{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-12th-week-셰익스피어글생성_NLG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2xFG4Xoxg2QtEbURRR4Vd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykkim77/nlp-12th/blob/main/nlp_12th_week_%EC%85%B0%EC%9D%B5%EC%8A%A4%ED%94%BC%EC%96%B4%EA%B8%80%EC%83%9D%EC%84%B1_NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_656zGZKA7Rt",
        "outputId": "a9804bc7-3154-472b-a6df-324002cefea8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt','shakespeare.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('shakespeare.txt', <http.client.HTTPMessage at 0x7f2817a3af10>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG8wUTQXBgzu",
        "outputId": "59665a52-1e3c-49e4-8924-638a920b5305"
      },
      "source": [
        "path = 'shakespeare.txt'\n",
        "\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print(text[:200])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLLAWsmpC-8X",
        "outputId": "5cf312df-2bb9-4196-a1e2-ef8fdf3f23ec"
      },
      "source": [
        "print('고유한 단어 종류',len(set(text)))\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(vocab[:10])\n",
        "print(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고유한 단어 종류 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsfnernvDsMI",
        "outputId": "27ea847f-5fb4-4fc0-d1f1-38a4a4ea4377"
      },
      "source": [
        "# word 2 number\n",
        "char2num = {}\n",
        "num2char = {}\n",
        "for ind,char in enumerate(set(text)):\n",
        "    char2num[char] = ind\n",
        "    num2char[ind] = char\n",
        "\n",
        "\n",
        "char2num['C']\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghbopLNNEfx2"
      },
      "source": [
        "#text to number\n",
        "encoded_text = []\n",
        "for char in text:\n",
        "    encoded_text.append(char2num[char])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wSjHM4gFMEG"
      },
      "source": [
        "def windowed_dataset(series, window_size, shuffle_buffer, batch_size):\n",
        "    series = tf.expand_dims(series,-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size +1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda x: x.batch(window_size +1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds =ds.map(lambda x: (x[:-1], x[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWMrMBNGUyz"
      },
      "source": [
        "train_data = windowed_dataset(np.array(encoded_text),100, 10000, 64)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pUxqAAPIIGi"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4bXlkLYGlSF",
        "outputId": "1f23fe08-1382-47f8-d052-d53ab12f26a3"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "batch_size = 64\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                     tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "                     tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                     tf.keras.layers.Dense(vocab_size)\n",
        "                     ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15S8TDNI4x6"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFIlylRcLfhT"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['acc'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9nLwDCmQmFb"
      },
      "source": [
        "steps_per_epoch 을 1700\n",
        "eposch 을 10 이상으로 주어 학습해 보세요! (시간이 많이 소요된다는 점을 참고)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js-0i7tzL4Je",
        "outputId": "0074da14-3782-40ed-f806-441735556f6b"
      },
      "source": [
        "model.fit(train_data, epochs= 1, steps_per_epoch=10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 77s 8s/step - loss: 2.3372 - acc: 0.3397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f280f16fb50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyFTd6OHRzvW"
      },
      "source": [
        "예측을 위한 모델 재정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQFZapOFL9qV"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_U81vRHR6C4"
      },
      "source": [
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGgvGr9ZR926",
        "outputId": "fd40d5b6-aa71-449b-fd7f-f1e45548b052"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgBeP6ARTdRE"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5kGGnvvThnw"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['acc'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek-EV-JjUL38"
      },
      "source": [
        "epoch을 10 이상,\n",
        "steps_per_epoch 을 1700이상 설정하면 정확도가 높아짐(수행 시간은 길어짐) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjhpzfqBTjX6",
        "outputId": "fe38ac57-e88a-426d-e24a-e2fedcadbb43"
      },
      "source": [
        "model.fit(train_data, \n",
        "          epochs=1, \n",
        "          steps_per_epoch=50, \n",
        "          )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 383s 8s/step - loss: 2.3943 - acc: 0.3344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f28111b7610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-24rDUHJTm-_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}